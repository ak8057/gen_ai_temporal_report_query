# Query Based Reports ğŸ“Š

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.8+-green.svg)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-20.10+-blue.svg)](https://www.docker.com/)
[![SQL](https://img.shields.io/badge/Database-PostgreSQL%20%7C%20MySQL-orange.svg)](https://www.postgresql.org/)

> An intelligent data ingestion and analysis system that transforms Excel files into queryable SQL databases with AI-powered insights capabilities.

## ğŸ¯ Overview

**Query Based Reports** is a comprehensive data processing pipeline designed to streamline the journey from raw Excel data to intelligent, AI-powered analytics. The system combines automated data ingestion with advanced generative AI capabilities to provide seamless data analysis workflows.

### Key Features

- **ğŸ”„ Automated Data Ingestion**: Seamlessly process Excel files and transform them into structured SQL database tables
- **ğŸ¤– AI-Powered Querying**: Leverage Generative AI (GEMINI) for intelligent data analysis and insights
- **ğŸ³ Containerized Architecture**: Docker-based deployment for consistent environments
- **ğŸ“ˆ Interactive Analysis**: Jupyter notebook interface for exploratory data analysis
- **ğŸ”§ Flexible Database Support**: Compatible with PostgreSQL and MySQL databases
- **âš¡ Scalable Processing**: Handle large datasets with optimized data processing pipelines

## ğŸ—ï¸ System Architecture

```
DataRetrievalintoSQL/
â”œâ”€â”€ ğŸ“ DATA/                          # Centralized data storage
â”œâ”€â”€ ğŸ“ feature_ingestion/             # Data ingestion microservice
â”‚   â”œâ”€â”€ ğŸ“ app/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ venv/                  # Isolated Python environment
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ ingest.py              # Core ingestion engine
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt       # Dependency specifications
â”‚   â”‚   â””â”€â”€ ğŸ“„ [Excel files]          # Source data files
â”‚   â”œâ”€â”€ ğŸ“„ .env                       # Environment configuration
â”‚   â””â”€â”€ ğŸ“„ docker-compose.yml         # Container orchestration
â”œâ”€â”€ ğŸ“ queryGeneration/               # AI analysis interface
â”‚   â””â”€â”€ ğŸ“„ sql_query.ipynb           # Interactive query notebook
â””â”€â”€ ğŸ“„ README.md                     # Project documentation
```

## ğŸ› ï¸ Technology Stack

### Core Dependencies

| Technology | Purpose | Version |
|------------|---------|---------|
| **pandas** | Data manipulation and analysis | Latest |
| **SQLAlchemy** | SQL toolkit and ORM | Latest |
| **openpyxl** | Excel file processing | Latest |
| **psycopg2-binary** | PostgreSQL database adapter | Latest |
| **PyMySQL** | MySQL database connector | Latest |
| **python-dotenv** | Environment variable management | Latest |

### Infrastructure & Tools

- **ğŸ³ Docker**: Containerized deployment and environment isolation
- **ğŸ—„ï¸ PostgreSQL/MySQL**: Robust relational database storage
- **ğŸ““ Jupyter Notebooks**: Interactive data analysis and visualization
- **ğŸ¤– GEMINI API**: Advanced generative AI for intelligent querying
- **ğŸ Python 3.8+**: Core programming language

## ğŸš€ Quick Start Guide

### Prerequisites

Ensure you have the following installed on your system:

- **Docker** (version 20.10+)
- **Docker Compose** (version 1.29+)
- **Python** (version 3.8+)
- **SQL Database** (PostgreSQL or MySQL)

### Installation

1. **Clone the Repository**
   ```bash
   git clone https://github.com/your-username/DataRetrievalintoSQL.git
   cd DataRetrievalintoSQL
   ```

2. **Environment Configuration**
   ```bash
   cd feature_ingestion
   cp .env.example .env
   # Configure your database credentials in .env file
   ```

3. **Launch the Application**
   ```bash
   docker-compose up -d
   ```

## ğŸ“‹ Usage Instructions

### Phase 1: Data Ingestion

#### Step 1: Prepare Your Data
- Place Excel files in the `/feature_ingestion/app/` directory
- Ensure database connection parameters are correctly configured in `.env`
- Verify file format compatibility (`.xlsx`, `.xls`)

#### Step 2: Execute Ingestion Process
```bash
cd feature_ingestion/app

# Activate virtual environment (if not using Docker)
source venv/bin/activate  # Linux/macOS
# or
venv\Scripts\activate     # Windows

# Run the ingestion script
python ingest.py
```

**What happens during ingestion:**
- âœ… Excel file parsing and validation
- âœ… Automatic schema detection and creation
- âœ… Data type inference and optimization
- âœ… SQL table generation with proper indexing
- âœ… Data integrity checks and error handling

### Phase 2: AI-Powered Analysis

#### Step 1: Launch Jupyter Environment
```bash
cd queryGeneration
jupyter notebook sql_query.ipynb
```

#### Step 2: Execute Intelligent Queries
- Use natural language queries through the GEMINI API integration
- Generate complex SQL queries with AI assistance
- Visualize results with built-in plotting capabilities

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in the `feature_ingestion` directory:

```bash
# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=your_database_name
DB_USER=your_username
DB_PASSWORD=your_password
DB_TYPE=postgresql  # or mysql

# AI API Configuration
GEMINI_API_KEY=your_gemini_api_key
GEMINI_MODEL=gemini-pro

# Application Settings
DEBUG=True
LOG_LEVEL=INFO
```

### Docker Configuration

The `docker-compose.yml` file includes:
- Application container with Python environment
- Database container (PostgreSQL/MySQL)
- Volume mappings for persistent data storage
- Network configuration for service communication

## ğŸ“Š Features & Capabilities

### Data Processing Features
- **Multi-format Support**: Excel (.xlsx, .xls), CSV files
- **Schema Auto-detection**: Automatic column type inference
- **Data Validation**: Built-in data quality checks
- **Batch Processing**: Handle multiple files simultaneously
- **Error Recovery**: Robust error handling and logging

### AI Query Features
- **Natural Language Processing**: Query data using plain English
- **SQL Generation**: AI-assisted query construction
- **Data Insights**: Automated pattern recognition and analysis
- **Visualization**: Interactive charts and graphs
- **Export Capabilities**: Multiple output formats (PDF, CSV, JSON)

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit your changes**: `git commit -m 'Add amazing feature'`
4. **Push to the branch**: `git push origin feature/amazing-feature`
5. **Open a Pull Request**

### Development Setup

```bash
# Clone your fork
git clone https://github.com/your-username/DataRetrievalintoSQL.git

# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/
```

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support & Documentation

- **ğŸ“š Documentation**: [Wiki Pages](https://github.com/your-username/DataRetrievalintoSQL/wiki)
- **ğŸ› Issues**: [GitHub Issues](https://github.com/your-username/DataRetrievalintoSQL/issues)
- **ğŸ’¬ Discussions**: [GitHub Discussions](https://github.com/your-username/DataRetrievalintoSQL/discussions)
- **ğŸ“§ Contact**: [your-email@domain.com](mailto:your-email@domain.com)

## ğŸ—ºï¸ Roadmap

- [ ] **v2.0**: Support for additional file formats (JSON, Parquet)
- [ ] **v2.1**: Real-time data streaming capabilities
- [ ] **v2.2**: Advanced AI model integration (GPT-4, Claude)
- [ ] **v2.3**: Web-based dashboard interface
- [ ] **v2.4**: Multi-tenant architecture support

## ğŸ“ˆ Performance Metrics

- **Processing Speed**: Up to 1M+ rows per minute
- **File Size Support**: Files up to 500MB
- **Database Compatibility**: PostgreSQL 12+, MySQL 8.0+
- **Memory Efficiency**: Optimized for low-memory environments
- **Scalability**: Horizontal scaling support with Docker Swarm

---
loyment
- **SQL Database** - Data storage (PostgreSQL/MySQL)
- **Jupyter Notebooks** - Interactive analysis interface
- **Generative AI API(GEMINI Currently)** - Intelligent query processing

## ğŸš€ Getting Started

### Prerequisites
- Docker and Docker Compose
- Python 3.x
- SQL Database (PostgreSQL or MySQL)

### Installation & Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd DataRetrievalintoSQL
   ```

2. **Configure environment**
   ```bash
   cd feature_ingestion
   # Edit .env file with your database credentials
   ```

3. **Start the ingestion service**
   ```bash
   docker-compose up
   ```

## ğŸ“Š Usage Workflow

### Step 1: Data Ingestion

1. **Prepare your data file**
   - Place your Excel file in `/feature_ingestion/app/` directory
   - Ensure SQL database connection is properly configured

2. **Run the ingestion process**
   ```bash
   cd feature_ingestion/app
   python ingest.py
   ```
   
   This will:
   - Process the Excel file
   - Create a dedicated table for the file
   - Store structured data with proper column mapping
